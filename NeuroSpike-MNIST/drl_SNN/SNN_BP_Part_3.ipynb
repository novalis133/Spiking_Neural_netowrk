{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "private_outputs": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# First Method\n",
    "\n",
    "- Author: Osama Abdelaal\n",
    "- Date: 2023-06-30     \n",
    "Be sure that you installed SnnTorch and Gym"
   ],
   "metadata": {
    "id": "n3Gu1Dahy_ED"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install snntorch"
   ],
   "metadata": {
    "id": "M31dCx8kcllR",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob\n",
    "\n",
    "torch.manual_seed(0)\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "class S_Policy(nn.Module):\n",
    "    def __init__(self, num_inputs=4, num_hidden=32, num_outputs=2):\n",
    "        super().__init__()\n",
    "        # Network Architecture\n",
    "        # self.num_steps = 25\n",
    "        beta = 0.95\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        #for step in range(self.num_steps):\n",
    "        cur1 = self.fc1(x)\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "        cur2 = self.fc2(spk1)\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        spk2_rec.append(spk2)\n",
    "        mem2_rec.append(mem2)\n",
    "        #print(spk2_rec)\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "    # Function to select an action\n",
    "    def act(self, state, temperature=1.0):\n",
    "        \"\"\"\n",
    "        This method selects an action based on the state.\n",
    "\n",
    "        Args:\n",
    "        - state: The current state of the environment\n",
    "        - temperature (float, optional): Temperature parameter for the softmax function to control\n",
    "        exploration-exploitation balance. It can be any positive real number, typically around 1.0.\n",
    "        High temperature (greater than 1.0) leads to more exploration (actions have similar probability),\n",
    "        and low temperature (less than 1.0) leads to more exploitation (the action with the highest\n",
    "        original probability is more likely to be chosen).\n",
    "\n",
    "        Returns:\n",
    "        - action (int): The selected action.\n",
    "        - action_dist.log_prob(action) (Tensor): The log probability of the selected action.\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # Prepare state for network input\n",
    "        action_probs, _ = self.forward(state) # Get action probabilities\n",
    "        action_probs = action_probs.squeeze(0)\n",
    "\n",
    "        # Adjust action probabilities using temperature and create a categorical distribution\n",
    "        action_dist = Categorical(F.softmax(action_probs / temperature, dim=-1))\n",
    "\n",
    "        action = action_dist.sample() # Sample an action\n",
    "        return action.item(), action_dist.log_prob(action) # Return the action and the log probability\n"
   ],
   "metadata": {
    "id": "XDOc7ql6vSQz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import backprop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RL_Dataset(Dataset):\n",
    "    \"\"\" PyTorch Dataset for offline RL \"\"\"\n",
    "\n",
    "    def __init__(self, states, actions, rewards, next_states, dones):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        self.dones = dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the items to tensors before returning\n",
    "        state = torch.tensor(self.states[idx]).float()\n",
    "        reward = torch.tensor(self.rewards[idx]).float()\n",
    "        next_state = torch.tensor(self.next_states[idx]).float()\n",
    "        done = torch.tensor(self.dones[idx]).float()\n",
    "\n",
    "        # Create a single data tensor by concatenating other tensors\n",
    "        data = torch.cat((state, reward.unsqueeze(0), next_state, done.unsqueeze(0)))\n",
    "\n",
    "        # Convert the action to a tensor and return it as the target\n",
    "        target = torch.tensor(self.actions[idx]).long()  # Assuming the action is an integer\n",
    "\n",
    "        return data, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train the policy using reinforce\n",
    "def reinforce(policy,BP_policy, policy_optimizer, snn_optimizer, n_episodes=1000, max_t=100, gamma=1.0, print_every=100):\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    policy (Policy): The policy to train.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer to use for training the policy.\n",
    "    n_episodes (int, optional): The maximum number of training episodes. Default is 1000.\n",
    "    max_t (int, optional): The maximum number of timesteps per episode. Default is 1000.\n",
    "    gamma (float, optional): The discount factor. Default is 1.0.\n",
    "    print_every (int, optional): How often to print average score. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    scores (List[float]): A list of scores from each episode of the training. The score is the total reward obtained in the episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a double-ended queue to hold the most recent 100 episode scores\n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    # List to store all episode scores\n",
    "    scores = []\n",
    "    scores = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    loss_fn = SF.mse_count_loss()\n",
    "    reg_fn = SF.l1_rate_sparsity()\n",
    "    # Loop over each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # List to save log probabilities for each step of this episode\n",
    "        saved_log_probs = []\n",
    "\n",
    "        # List to save rewards for each step of this episode\n",
    "        episode_rewards = []\n",
    "\n",
    "        # Reset the environment and get initial state\n",
    "        state = env.reset(seed=0)\n",
    "\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Use the policy to select an action given the current state\n",
    "            action, log_prob = policy.act(state)\n",
    "\n",
    "            # Save the log probability of the chosen action\n",
    "            saved_log_probs.append(log_prob)\n",
    "\n",
    "            # Take the action and get the new state and reward\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Add the reward to the list of rewards for this episode\n",
    "            episode_rewards.append(reward)\n",
    "            # Store experience\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(state_)\n",
    "            dones.append(done)\n",
    "            # If the episode is done, break out of the loop\n",
    "            if done:\n",
    "                break\n",
    "            state = state_\n",
    "        # Calculate total reward for this episode and add it to the deque and list of scores\n",
    "        scores_deque.append(sum(episode_rewards))\n",
    "        scores.append(sum(episode_rewards))\n",
    "\n",
    "        # Compute future discount rewards for each step\n",
    "        discounts = [gamma**i for i in range(len(episode_rewards)+1)]\n",
    "\n",
    "        # Calculate total discounted reward for the episode\n",
    "        R = sum([a*b for a, b in zip(discounts, episode_rewards)])\n",
    "\n",
    "        # Compute the policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R) # note that gradient ascent is the same as gradient descent with negative rewards\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Creating the dataset\n",
    "        dataset = RL_Dataset(states, actions, rewards, next_states, dones)\n",
    "        # Create a dataloader\n",
    "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        # Backprobagate trhoough time\n",
    "        loss = backprop.BPTT(BP_policy, dataloader, optimizer=snn_optimizer,\n",
    "                             criterion=loss_fn, num_steps=1, time_var=False,\n",
    "                             regularization=reg_fn, device=device)\n",
    "\n",
    "\n",
    "        # Perform a step of policy gradient descent\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Clear the computation graph\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "        # Perform a step of SNN optimization\n",
    "        #snn_optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #snn_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print current average score every 'print_every' episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}\\tSNN Loss Score: {:.2f}'.format(i_episode,\n",
    "                                                                                     np.mean(scores_deque),\n",
    "                                                                                     loss))\n",
    "\n",
    "\n",
    "        # Stop if the environment is solved\n",
    "        if np.mean(scores_deque)>=500.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "\n",
    "    # Return all episode scores\n",
    "    return scores"
   ],
   "metadata": {
    "id": "L2S1KSciJks3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Main\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import backprop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "lif1 = snn.Leaky(beta=0.9, init_hidden=True)\n",
    "lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True)\n",
    "\n",
    "s_policy = S_Policy()\n",
    "policy_optimizer = optim.Adam(s_policy.parameters(), lr=1e-2)\n",
    "\n",
    "BP_net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(10,500),\n",
    "                    lif1,\n",
    "                    nn.Linear(500, 2),\n",
    "                    lif2).to(device)\n",
    "\n",
    "snn_optimizer = optim.Adam(BP_net.parameters(), lr=1e-3)  # Notice the different learning rate\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "lif1 = snn.Leaky(beta=0.9, init_hidden=True)\n",
    "lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True)\n",
    "\n",
    "BP_net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(10,500),\n",
    "                    lif1,\n",
    "                    nn.Linear(500, 2),\n",
    "                    lif2).to(device)\n",
    "\n",
    "scores = reinforce(s_policy, BP_net, policy_optimizer, snn_optimizer)\n"
   ],
   "metadata": {
    "id": "aHAKDNO_Jn9Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Second Method\n",
    "\n",
    "- Author: Osama Abdelaal\n",
    "- Date: 2023-06-30     \n",
    "Be sure that you installed SnnTorch and Gym"
   ],
   "metadata": {
    "id": "gFE5HRsuzJl1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Description: Policy network for the SNN agent\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class S_Policy(nn.Module):\n",
    "    def __init__(self, num_inputs=4, num_hidden=32, num_outputs=2, ):\n",
    "        super().__init__()\n",
    "        # Network Architecture\n",
    "        # self.num_steps = 25\n",
    "        beta = 0.95\n",
    "        self.device = torch.device( \"cpu\" )\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        #for step in range(self.num_steps):\n",
    "        cur1 = self.fc1(x)\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "        cur2 = self.fc2(spk1)\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        spk2_rec.append(spk2)\n",
    "        mem2_rec.append(mem2)\n",
    "        #print(spk2_rec)\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "    # Function to select an action\n",
    "    def act(self, state, temperature=1.0):\n",
    "        \"\"\"\n",
    "        This method selects an action based on the state.\n",
    "\n",
    "        Args:\n",
    "        - state: The current state of the environment\n",
    "        - temperature (float, optional): Temperature parameter for the softmax function to control\n",
    "        exploration-exploitation balance. It can be any positive real number, typically around 1.0.\n",
    "        High temperature (greater than 1.0) leads to more exploration (actions have similar probability),\n",
    "        and low temperature (less than 1.0) leads to more exploitation (the action with the highest\n",
    "        original probability is more likely to be chosen).\n",
    "\n",
    "        Returns:\n",
    "        - action (int): The selected action.\n",
    "        - action_dist.log_prob(action) (Tensor): The log probability of the selected action.\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device) # Prepare state for network input\n",
    "        action_probs, _ = self.forward(state) # Get action probabilities\n",
    "        action_probs = action_probs.squeeze(0)\n",
    "\n",
    "        # Adjust action probabilities using temperature and create a categorical distribution\n",
    "        action_dist = Categorical(F.softmax(action_probs / temperature, dim=-1))\n",
    "\n",
    "        action = action_dist.sample() # Sample an action\n",
    "        return action.item(), action_dist.log_prob(action) # Return the action and the log probability\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from snntorch import backprop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RL_Dataset(Dataset):\n",
    "    \"\"\" PyTorch Dataset for offline RL \"\"\"\n",
    "\n",
    "    def __init__(self, states, actions) :# rewards, next_states, dones):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        #self.rewards = rewards\n",
    "        #self.next_states = next_states\n",
    "        #self.dones = dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the items to tensors before returning\n",
    "        state = torch.tensor(self.states[idx]).float()\n",
    "        #reward = torch.tensor(self.rewards[idx]).float()\n",
    "        #next_state = torch.tensor(self.next_states[idx]).float()\n",
    "        #done = torch.tensor(self.dones[idx]).float()\n",
    "\n",
    "        # Create a single data tensor by concatenating other tensors\n",
    "        #data = torch.cat((state, reward.unsqueeze(0), next_state, done.unsqueeze(0)))\n",
    "\n",
    "        # Convert the action to a tensor and return it as the target\n",
    "        target = torch.tensor(self.actions[idx]).long()  # Assuming the action is an integer\n",
    "\n",
    "        return state, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to train the policy using reinforce\n",
    "def reinforce(policy,BP_policy, policy_optimizer, snn_optimizer,\n",
    "              env, device, n_episodes=500, max_t=10, gamma=1.0, print_every=100):\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    policy (Policy): The policy to train.\n",
    "    optimizer (torch.optim.Optimizer): The optimizer to use for training the policy.\n",
    "    n_episodes (int, optional): The maximum number of training episodes. Default is 1000.\n",
    "    max_t (int, optional): The maximum number of timesteps per episode. Default is 1000.\n",
    "    gamma (float, optional): The discount factor. Default is 1.0.\n",
    "    print_every (int, optional): How often to print average score. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    scores (List[float]): A list of scores from each episode of the training. The score is the total reward obtained in the episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a double-ended queue to hold the most recent 100 episode scores\n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    # List to store all episode scores\n",
    "    scores = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    loss_fn = SF.mse_count_loss()\n",
    "    reg_fn = SF.l1_rate_sparsity()\n",
    "    # Loop over each episode\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # List to save log probabilities for each step of this episode\n",
    "        saved_log_probs = []\n",
    "\n",
    "        # List to save rewards for each step of this episode\n",
    "        episode_rewards = []\n",
    "\n",
    "        # Reset the environment and get initial state\n",
    "        state = env.reset(seed=0)\n",
    "\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Use the policy to select an action given the current state\n",
    "            action, log_prob = policy.act(state)\n",
    "\n",
    "            # Save the log probability of the chosen action\n",
    "            saved_log_probs.append(log_prob)\n",
    "\n",
    "            # Take the action and get the new state and reward\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Add the reward to the list of rewards for this episode\n",
    "            episode_rewards.append(reward)\n",
    "            # Store experience\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(state_)\n",
    "            dones.append(done)\n",
    "            state = state_\n",
    "            # If the episode is done, break out of the loop\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Calculate total reward for this episode and add it to the deque and list of scores\n",
    "        scores_deque.append(sum(episode_rewards))\n",
    "        scores.append(sum(episode_rewards))\n",
    "\n",
    "        # Compute future discount rewards for each step\n",
    "        discounts = [gamma**i for i in range(len(episode_rewards)+1)]\n",
    "\n",
    "        # Calculate total discounted reward for the episode\n",
    "        R = sum([a*b for a, b in zip(discounts, episode_rewards)])\n",
    "\n",
    "        # Compute the policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R) # note that gradient ascent is the same as gradient descent with negative rewards\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        # Creating the dataset\n",
    "        dataset = RL_Dataset(states, actions)\n",
    "        # Create a dataloader\n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "        # Backprobagate trhoough time\n",
    "        loss= backprop.BPTT(BP_policy, dataloader, optimizer=snn_optimizer,\n",
    "                             criterion=loss_fn, num_steps=max_t, time_var=False,\n",
    "                             regularization=reg_fn, device=device)\n",
    "\n",
    "\n",
    "        # Perform a step of policy gradient descent\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Clear the computation graph\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "        # Perform a step of SNN optimization\n",
    "        #snn_optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #snn_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print current average score every 'print_every' episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}\\tSNN Loss Score: {:.2f}\\tPolicy Loss Score: {:.2f}'.format(i_episode,\n",
    "                                                                                                                np.mean(scores_deque),\n",
    "                                                                                                                loss,\n",
    "                                                                                                                policy_loss))\n",
    "\n",
    "        # Stop if the environment is solved\n",
    "        if np.mean(scores_deque)>=500.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "\n",
    "    # Return all episode scores\n",
    "    return scores"
   ],
   "metadata": {
    "id": "gJJPyDR1JqfW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Main\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#from s_policy import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "lif1 = snn.Leaky(beta=0.9, init_hidden=True)\n",
    "lif2 = snn.Leaky(beta=0.9, init_hidden=True, output=True)\n",
    "\n",
    "s_policy = S_Policy()\n",
    "policy_optimizer = optim.Adam(s_policy.parameters(), lr=1e-2)\n",
    "\n",
    "BP_net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(10,32),\n",
    "                    lif1,\n",
    "                    nn.Linear(32, 2),\n",
    "                    lif2).to(device)\n",
    "\n",
    "snn_optimizer = optim.Adam(BP_net.parameters(), lr=1e-3)  # Notice the different learning rate\n",
    "\n",
    "\n",
    "scores = reinforce(s_policy, s_policy, policy_optimizer, snn_optimizer, env, device)\n"
   ],
   "metadata": {
    "id": "sfZnoAAensAu"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
